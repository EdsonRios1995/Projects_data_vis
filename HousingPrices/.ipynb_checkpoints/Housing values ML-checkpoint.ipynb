{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Let's import all that will be needed\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Some methods useful for preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split, cross_val_score, GridSearchCV #For stratified sampling\n",
    "\n",
    "#The bases for our custom transformation\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "#Some models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#A metric for measuring RMSE\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We fetch our data from githubusercontent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "download_root = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "Housing_path = os.path.join(\"datasets\", \"housing\")\n",
    "Housing_url = download_root + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=Housing_url, housing_path=Housing_path):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "#Now the csv file is available for future usage\n",
    "fetch_housing_data()\n",
    "\n",
    "def load_housing_data(housing_path = Housing_path):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "#We load our data\n",
    "HousingData = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We split the whole data set for training purposes and then stratified our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(HousingData, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "HousingData[\"income_cat\"] = np.ceil(HousingData[\"median_income\"] / 1.5)\n",
    "HousingData[\"income_cat\"].where(HousingData[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "#We make stratification based on the income_cat that was obtained with median_income\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(HousingData, HousingData[\"income_cat\"]):\n",
    "    strat_train_set = HousingData.loc[train_index]\n",
    "    strat_test_set = HousingData.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now we remove \"income_cat\" after stratifying the set. So we get our set as before. \n",
    "\n",
    "And create a copy of our housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)\n",
    "\n",
    "housing = strat_train_set.copy()\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We separate our data into numerical and categorical and adapt the last one. \n",
    "\n",
    "Then we create a custom transform for adding extra attributes. and use the column transformer to transform categorical and numerical data correspondingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "housing_num = housing.drop(\"ocean_proximity\", axis = 1)\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 4,5,6,7\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "#Transformations for numerical data        \n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"attribs_adder\", CombinedAttributesAdder()),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "#We specify the attributes that are numerical and categorical\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "#Combining the previous transformation with the categorical one hot encoder\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's start training and evaluating on the training set.\n",
    "\n",
    "We use mean_squared_error to get this model's RMSE.\n",
    "\n",
    "# Linear Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69036.32451019084"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "#We gain an inside of prediction and real value\n",
    "#some_data = housing.iloc[:10]\n",
    "#some_labels = housing_labels.iloc[:10]\n",
    "#some_data_prepared = full_pipeline.transform(some_data)\n",
    "#for pred, real in zip(lin_reg.predict(some_data_prepared), list(some_labels)):\n",
    "#    print(\"Prediction :\", pred, \"- Real Value :\", real) \n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions))\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Decision Tree Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions_tree = tree_reg.predict(housing_prepared)\n",
    "tree_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions_tree))\n",
    "tree_rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Obviously there is some overfitting with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Random Forest Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19906.401154299754"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_reg = RandomForestRegressor(n_estimators = 30, max_features = 8)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions_forest = forest_reg.predict(housing_prepared)\n",
    "forest_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions_forest))\n",
    "forest_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Cross-validation for avoiding overfitting\n",
    "The previous result show us that there may be some data overfitting.\n",
    "\n",
    "A great alternative is to use Scikit-Learn’s cross-validation feature. The following code performs K-fold cross-validation: it randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 70651.99000254  68090.07442376  72413.80312079  74901.80329318\n",
      "  72313.32643138  75874.85666343  71522.11993936  69207.55485039\n",
      "  73840.6799095   70480.63436683]\n",
      "Mean: 71929.6843001\n",
      "Standard deviation: 2334.14401228\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We do the same for the linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 67469.28784592  67422.28193468  68356.19928133  74785.20131013\n",
      "  68241.17784274  71620.02931289  65379.21201448  68578.87113789\n",
      "  73052.42414895  68092.44973869]\n",
      "Mean: 69299.7134568\n",
      "Standard deviation: 2753.0098942\n"
     ]
    }
   ],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We do the same for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ 49450.25969937  48741.36900907  50861.65875388  52368.50742507\n",
      "  51449.82750641  54004.35372113  49856.02067728  50179.68751028\n",
      "  54161.69436916  51170.35539183]\n",
      "Mean: 51224.3734063\n",
      "Standard deviation: 1735.71441064\n"
     ]
    }
   ],
   "source": [
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We see that random Forest is actually the most accurate model for predicting housing costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50078.719873647897"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_param = {'C': 157055.10989448498, 'gamma': 0.26497040005002437, 'kernel': 'rbf'} #I got these values from doing randomized search\n",
    "VectorMach_reg = SVR(**best_param)\n",
    "VectorMach_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "housing_predictions_VectorMach = VectorMach_reg.predict(housing_prepared)\n",
    "VectorMach_rmse = np.sqrt(mean_squared_error(housing_labels, housing_predictions_VectorMach))\n",
    "VectorMach_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far SVM seems to be the best solution for predicting housing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=10, n_jobs=None,\n",
       "          param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000022AE03A49B0>, 'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000022AE03A4CF8>},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring='neg_mean_squared_error',\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "        'n_estimators': randint(low=1, high=200),\n",
    "        'max_features': randint(low=1, high=8),\n",
    "    }\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n",
    "                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "rnd_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50681.6021466 {'max_features': 7, 'n_estimators': 180}\n",
      "53434.1465117 {'max_features': 5, 'n_estimators': 15}\n",
      "52778.836354 {'max_features': 3, 'n_estimators': 72}\n",
      "52537.4679252 {'max_features': 5, 'n_estimators': 21}\n",
      "50762.126242 {'max_features': 7, 'n_estimators': 122}\n",
      "52748.7755502 {'max_features': 3, 'n_estimators': 75}\n",
      "52557.2964939 {'max_features': 3, 'n_estimators': 88}\n",
      "51520.3047696 {'max_features': 5, 'n_estimators': 100}\n",
      "52427.1765887 {'max_features': 3, 'n_estimators': 150}\n",
      "67348.1147123 {'max_features': 5, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "cvres = rnd_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We define the class for returning top k important features In case we want to build our model based on the top k important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def indices_of_top_k(arr, k):\n",
    "    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n",
    "\n",
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_importances, k):\n",
    "        self.feature = feature_importances\n",
    "        self.k = k\n",
    "    def fit(self, X, y = None):\n",
    "        self.feature_indices = indices_of_top_k(self.feature, self.k)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return(X[:,self.feature_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "Pipeline_important_features = Pipeline([\n",
    "    (\"full_pipeline\", full_pipeline),\n",
    "    (\"feature_selection\", FeatureSelector(feature_importances, k))\n",
    "])\n",
    "\n",
    "Housing_prepared_selectedfeatures = Pipeline_important_features.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962, -0.61493744, -0.07199305,  0.        ],\n",
       "       [-1.17602483,  0.6596948 ,  1.33645936, -0.00838466,  0.        ],\n",
       "       [ 1.18684903, -1.34218285, -0.5320456 , -0.07478138,  0.        ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Housing_prepared_selectedfeatures[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Full_pipeline implementation\n",
    "\n",
    "Creating a single pipeline that does the full data preparation plus the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('full_pipeline', ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n",
       "         transformer_weights=None,\n",
       "         transformers=[('num', Pipeline(memory=None,\n",
       "     steps=[('imputer', SimpleImputer(copy=True, fill_value=None, missing_values=nan,\n",
       "       strategy='median', verb... gamma=0.26497040005002437, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pipeline_important_features_prediction= Pipeline([\n",
    "    (\"full_pipeline\", full_pipeline),\n",
    "    (\"feature_selection\", FeatureSelector(feature_importances, k)),\n",
    "    (\"SVM_reg\", SVR(**best_param))\n",
    "])\n",
    "Pipeline_important_features_prediction.fit(housing, housing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions vs labels comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\t [ 197718.4844884   365651.12767866  170689.32595093   47239.82858247\n",
      "  203857.47654804  132870.50980247  481421.15585331  186148.43576607\n",
      "  131372.01843668   76677.79682102]\n",
      "Labels:\t\t [286600.0, 340600.0, 196900.0, 46300.0, 254500.0, 127900.0, 500001.0, 140200.0, 95000.0, 500001.0]\n"
     ]
    }
   ],
   "source": [
    "some_data = housing.iloc[:10]\n",
    "some_labels = housing_labels.iloc[:10]\n",
    "\n",
    "print(\"Predictions:\\t\", Pipeline_important_features_prediction.predict(some_data))\n",
    "print(\"Labels:\\t\\t\", list(some_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model is actually very accurate, compact version with only pipelines are to be done later on for this project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
